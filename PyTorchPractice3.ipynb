{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchPractice3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMvR8FSKYeilrPrf4qaqHmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0db298797a24582bf600b5e05ef1392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d529a789709e4d7e92bc06b8c7139186",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ea6da43491c48a984faf966f49437f6",
              "IPY_MODEL_2eb1653fe5ff4e3c86abaec583ede111"
            ]
          }
        },
        "d529a789709e4d7e92bc06b8c7139186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ea6da43491c48a984faf966f49437f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0673de043e2485bbafdcd8a7c47af63",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1743fe90ac6b4754b0e4e86211cfe229"
          }
        },
        "2eb1653fe5ff4e3c86abaec583ede111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a9c65e1b55f840a38bd99343bd6d8619",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:01&lt;00:00, 133MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_437dc9294fa54fd8bda8814b7ac98884"
          }
        },
        "e0673de043e2485bbafdcd8a7c47af63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1743fe90ac6b4754b0e4e86211cfe229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9c65e1b55f840a38bd99343bd6d8619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "437dc9294fa54fd8bda8814b7ac98884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b3ecb3a83914fcfb55775efbe8b796c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_77cba952034943f8a505945769231ddd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6aa761d99ba040d9b98469e6066f726b",
              "IPY_MODEL_95d7502bdc364193a76f9b43f5261cac"
            ]
          }
        },
        "77cba952034943f8a505945769231ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6aa761d99ba040d9b98469e6066f726b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8b8b6917264d48c9a1e4e038e8d6c6ea",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe9185a0c9fd4b01920831fff9f186b4"
          }
        },
        "95d7502bdc364193a76f9b43f5261cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1993afbcbf74c93b1ada4d9f283300b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 46.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6409bf60429f4369b2248b301db6a4c2"
          }
        },
        "8b8b6917264d48c9a1e4e038e8d6c6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe9185a0c9fd4b01920831fff9f186b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1993afbcbf74c93b1ada4d9f283300b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6409bf60429f4369b2248b301db6a4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kweenkeen/PyTorchPractice/blob/master/PyTorchPractice3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAiGC39IHxy",
        "colab_type": "text"
      },
      "source": [
        "# TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL\n",
        "\n",
        "For this tutorial, we will be finetuning a pre-trained Mask R-CNN model in the Penn-Fudan Database for Pedestrial Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.\n",
        "\n",
        "### Defining the Dataset\n",
        "\n",
        "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
        "\n",
        "The only specificity that we require is that the dataset `__getitem__` should return:\n",
        "\n",
        "*   image: a PIL Image of size **`(H, W)`**\n",
        "*   target: a dict containing the following fields\n",
        "  * **`boxes`** (FloatTensor[N,4]): the coordinates of the **`N`**  bounding boxes in **`[x0, y0, x1, y1]`** format, ranging from **`0`** to **`W`** and **`0`** to **`H`**\n",
        "  * **`labels (Int64Tensor[N])`**: the label for each bounding box. **`0`** represents always the background class\n",
        "  * **`image_id (Int64Tensor[1]`**: an image identifier. It should be unique between all the images in the dataset and is used during evaluation.\n",
        "  * **`area (Tensor[N])`**: The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium, and large boxes.\n",
        "  * **`iscrowd (UInt8Tensor[N])`**: instances wih iscrowd=True will be ignored during evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hss8_dJNH7HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "  def __init__(self, root, transforms):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    # load images and masks\n",
        "    img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # note that wehaven't converted the mask to RGB\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "    mask = Image.open(mask_path)\n",
        "    # convert the PIL Image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "      pos = np.where(masks[i])\n",
        "      xmin = np.min(pos[1])\n",
        "      xmax = np.max(pos[1])\n",
        "      ymin = np.min(pos[0])\n",
        "      ymax = np.max(pos[0])\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs), dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:,2] - boxes[:, 0])\n",
        "    # suppose all instances are now crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "    return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.imgs)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqvYAzjCOVkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AWZ8NvjO_Ye",
        "colab_type": "text"
      },
      "source": [
        "### Defining your model\n",
        "\n",
        "In this tutorial, we will be using Mask R-CNN, which is based on top of Faster R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
        "\n",
        "There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
        "\n",
        "#### 1. Finetuning from a pretrained model\n",
        "\n",
        "Let's suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yTMfHR6PeaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9xA-We1Puqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "c0db298797a24582bf600b5e05ef1392",
            "d529a789709e4d7e92bc06b8c7139186",
            "4ea6da43491c48a984faf966f49437f6",
            "2eb1653fe5ff4e3c86abaec583ede111",
            "e0673de043e2485bbafdcd8a7c47af63",
            "1743fe90ac6b4754b0e4e86211cfe229",
            "a9c65e1b55f840a38bd99343bd6d8619",
            "437dc9294fa54fd8bda8814b7ac98884"
          ]
        },
        "outputId": "8cf2b7de-bf4b-457d-c891-053ee579e1db"
      },
      "source": [
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0db298797a24582bf600b5e05ef1392",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20CZNhVkP8pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace the classifier with a new one that has num_classes\n",
        "# which is user-defined\n",
        "num_classes = 2 # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rXUp_GdQdRt",
        "colab_type": "text"
      },
      "source": [
        "### 2 - Modifying the model to add a different backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33QyQJCCQaxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "4b3ecb3a83914fcfb55775efbe8b796c",
            "77cba952034943f8a505945769231ddd",
            "6aa761d99ba040d9b98469e6066f726b",
            "95d7502bdc364193a76f9b43f5261cac",
            "8b8b6917264d48c9a1e4e038e8d6c6ea",
            "fe9185a0c9fd4b01920831fff9f186b4",
            "b1993afbcbf74c93b1ada4d9f283300b",
            "6409bf60429f4369b2248b301db6a4c2"
          ]
        },
        "outputId": "0cedafea-a344-48ad-e9eb-fcac87319a6e"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect ratios.\n",
        "# We have a Tuple[Tuple[int]] because each feature map could\n",
        "# potentially have different sizes and aspect ratios.\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                  aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will use to perform\n",
        "# the region of interest cropping, as well as the size of the crop after\n",
        "# rescaling.\n",
        "# If your backbone returns a Tensor, featmap_names is expected to be [0].\n",
        "# More generally, the backbone should return an OrderedDict[Tensor], and\n",
        "# in featmap_names you can choose which feature maps to use.\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b3ecb3a83914fcfb55775efbe8b796c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWckA7wKSPu7",
        "colab_type": "text"
      },
      "source": [
        "#### An Instance Segmentation model for PennFudan Dataset\n",
        "\n",
        "In our case, we want to fine-tune from a pre-trained model, given hat our dataset is very small, so we will be following approach number 1.\n",
        "\n",
        "Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qksMF7AzRYTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "  # load an instance segmentation model pre-trained on COCO\n",
        "  model = torchdivision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_head.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  # now get the number of input features for the mask classifier\n",
        "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "  hidden_layer = 256\n",
        "  # and replace the mask predictor with a new one\n",
        "  model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                     hidden_layer,\n",
        "                                                     num_classes)\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sssZ1w0S-IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}